name: Daily Quiz Scraper

on:
  schedule:
    # Run at 9:00 AM IST (3:00 AM UTC) every day
    - cron: '0 3 * * *'
  workflow_dispatch:  # Allow manual triggering

jobs:
  scrape-and-send:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          
      - name: Install Python dependencies
        run: |
          pip install -r requirements.txt
      
      - name: Install system dependencies for Playwright
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libasound2t64 \
            libatk-bridge2.0-0 \
            libatk1.0-0 \
            libatspi2.0-0 \
            libcairo2 \
            libcups2 \
            libdbus-1-3 \
            libdrm2 \
            libgbm1 \
            libglib2.0-0 \
            libnspr4 \
            libnss3 \
            libpango-1.0-0 \
            libx11-6 \
            libxcb1 \
            libxcomposite1 \
            libxdamage1 \
            libxext6 \
            libxfixes3 \
            libxkbcommon0 \
            libxrandr2 \
            libxshmfence1 \
            fonts-liberation \
            libappindicator3-1 \
            xdg-utils
      
      - name: Install Playwright browsers
        run: |
          python -m playwright install chromium
      
      - name: Install Node.js dependencies
        run: |
          npm install
      
      - name: Install Playwright browsers for Node.js
        run: |
          npx playwright install chromium
      
      - name: Verify Playwright installation
        run: |
          python -c "from playwright.sync_api import sync_playwright; print('Playwright Python package OK')"
          echo "Playwright installed successfully"
          
      - name: Run scraper
        env:
          LOGIN_EMAIL: ${{ secrets.LOGIN_EMAIL }}
          LOGIN_PASSWORD: ${{ secrets.LOGIN_PASSWORD }}
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHANNEL: ${{ secrets.TELEGRAM_CHANNEL || '@currentadda' }}
          TELEGRAM_TEXT_CHANNEL: ${{ secrets.TELEGRAM_TEXT_CHANNEL || '' }}
          GIST_TOKEN: ${{ secrets.GIST_TOKEN }}
          GIST_ID: ${{ secrets.GIST_ID }}
          SESSION_GIST_ID: ${{ secrets.SESSION_GIST_ID }}
        run: |
          python -m src.runner
          
      - name: Commit updated tracking file (backup)
        # This step is optional if using GitHub Gist for state management
        # It provides a local backup of the tracking file
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add data/scraped_urls.json data/session.json || true
          git diff --quiet && git diff --staged --quiet || git commit -m "Update scraped URLs and session [skip ci]" || true
          git push || true
        continue-on-error: true
